# src/data/face_dataset.py
# ------------------------------------------------------------------
# Face Super-Resolution Dataset Loader for EFANet
# Supports aligned facial image datasets such as CelebA, Helen, FFHQ.
# Automatically handles downsampling for target scale (×4/×8/×16).
# ------------------------------------------------------------------

import os
import glob
from PIL import Image
from typing import Tuple

import torch
from torch.utils.data import Dataset
import torchvision.transforms as T
import torchvision.transforms.functional as TF


class FaceSuperResolutionDataset(Dataset):
    """
    Dataset for aligned face super-resolution (CelebA, Helen, FFHQ).

    Each sample returns a low-resolution (LR) input and its corresponding
    high-resolution (HR) target. LR is generated by downsampling HR using bicubic interpolation.

    Parameters
    ----------
    root_dir : str
        Path to dataset directory (expects 'train'/'val'/'test' subfolders).
    phase : str, default='train'
        Split to use: 'train' | 'val' | 'test'.
    scale : int, default=4
        Downsampling scale (×4, ×8, ×16).
    crop_size : int, optional
        Optional HR crop size (only used in training).
    """

    def __init__(self, root_dir: str, phase: str = 'train', scale: int = 4, crop_size: int = None):
        assert phase in ['train', 'val', 'test'], f"Invalid phase: {phase}"
        assert scale in [4, 8, 16], "Only ×4, ×8, ×16 scales supported"

        self.scale = scale
        self.phase = phase
        self.crop_size = crop_size
        self.hr_dir = os.path.join(root_dir, phase)
        self.image_paths = sorted(glob.glob(os.path.join(self.hr_dir, '*.jpg')) +
                                  glob.glob(os.path.join(self.hr_dir, '*.png')))

        self.to_tensor = T.ToTensor()
        self.to_pil = T.ToPILImage()

    def __len__(self) -> int:
        return len(self.image_paths)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        img_path = self.image_paths[idx]
        hr_img = Image.open(img_path).convert('RGB')

        if self.phase == 'train' and self.crop_size:
            hr_img = self._random_crop(hr_img, self.crop_size)

        lr_img = hr_img.resize(
            (hr_img.width // self.scale, hr_img.height // self.scale),
            resample=Image.BICUBIC
        )

        hr_tensor = self.to_tensor(hr_img)
        lr_tensor = self.to_tensor(lr_img)

        return {'lr': lr_tensor, 'hr': hr_tensor}

    def _random_crop(self, img: Image.Image, size: int) -> Image.Image:
        w, h = img.size
        if w < size or h < size:
            raise ValueError(f"Crop size {size} too large for image of size {w}x{h}")
        left = torch.randint(0, w - size + 1, (1,)).item()
        top = torch.randint(0, h - size + 1, (1,)).item()
        return TF.crop(img, top, left, size, size)
